{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfBL+n3/t+Kkc9yjA/ghR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamwq/NDVI/blob/main/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCAIb_oFEZl7"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created in 2017\n",
        "\n",
        "@author: mass.gargiulo & anto.mazza\n",
        "\"\"\"\n",
        "\n",
        "#############\n",
        "#   A CNN-Based Fusion Method for Feature Extraction from Sentinel Data\n",
        "\n",
        "#(http://www.mdpi.com/2072-4292/10/2/236) \n",
        "#############\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "from funct_Train import *\n",
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import lasagne\n",
        "import scipy.io as sio\n",
        "\n",
        "\n",
        "\n",
        "def main(data_folder, output_folder, identifier, num_epochs=1500):\n",
        "\n",
        "    # Hyper-parameters\n",
        "    t0,tt0,v0,rate= 0,0,0,0.5*10**(-2)\n",
        "\n",
        "\n",
        "    ps = 33  # patch (linear) size\n",
        "    k_1 = 9  # receptive field side - layer 1\n",
        "    k_2 = 5  # receptive field side - layer 2\n",
        "    k_3 = 5  # receptive field side - layer 3\n",
        "    \n",
        "    r = ((k_1 - 1) + (k_2 - 1) + (k_3 - 1)) / 2\n",
        "    ########################################################\n",
        "\n",
        "        # Load the dataset\n",
        "    print(\"Loading data...\")\n",
        "    \n",
        "    num = 1\n",
        "    X_train, y_train, X_val, y_val = load_dataset(data_folder, ps, r, identifier,num) \n",
        "\n",
        "\n",
        "    # Prepare Theano variables for inputs and targets\n",
        "    input_var = T.tensor4('inputs')\n",
        "    target_var = T.tensor4('targets') \n",
        "\n",
        "    \n",
        "    # Model building\n",
        "    print(\"Building model and compiling functions...\")\n",
        "    network = build_cnn(input_var,X_train.shape[1])\n",
        "    \n",
        "    # Create loss for training\n",
        "    prediction = lasagne.layers.get_output(network)\n",
        "    \n",
        "    loss = abs(prediction-target_var)\n",
        "    loss = loss.mean()\n",
        "    # We could add some weight decay as well here, see lasagne.regularization.\n",
        "\n",
        "    # Create update expressions for training\n",
        "    # Here, we'll use Stochastic Gradient Descent (SGD) with Nesterov momentum\n",
        "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
        "    l_rate = T.scalar('learn_rate','float32')\n",
        "    updates = lasagne.updates.momentum(loss, params, l_rate, momentum=0.9)\n",
        "    \n",
        "    # Create a loss expression for validation/testing. The crucial difference here is\n",
        "    # that we do a deterministic forward pass through the network, disabling dropout layers.\n",
        "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
        "    test_loss = abs(test_prediction-target_var)\n",
        "    test_loss = test_loss.mean()\n",
        "\n",
        "    # Compile a function performing a training step on a mini-batch (by giving\n",
        "    # the updates dictionary) and returning the corresponding training loss:8\n",
        "    train_fn = theano.function([input_var,target_var, l_rate], loss, updates=updates)\n",
        "\n",
        "    # Compile a second function computing the validation loss and accuracy:\n",
        "    val_fn = theano.function([input_var, target_var], test_loss)  \n",
        "    \n",
        "    # Finally, launch the tcraining loop.\n",
        "    print(\"Starting training...\")\n",
        "    train_loss_curve = []\n",
        "    val_loss_curve = []\n",
        "\n",
        "    # We iterate over epochs:\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "            # In each epoch, we do a full pass over the training data:\n",
        "        train_err = 0\n",
        "        train_batches = 0\n",
        "        start_time = time.time()\n",
        "        for batch in iterate_minibatches(X_train, y_train, 128, shuffle=False):\n",
        "            inputs, targets = batch\n",
        "            train_err += train_fn(inputs, targets,rate)\n",
        "            train_batches += 1\n",
        "\n",
        "            # And a full pass over the validation data:\n",
        "        val_err = 0\n",
        "        val_batches = 0\n",
        "        for batch in iterate_minibatches(X_val, y_val, 128, shuffle=False):\n",
        "            inputs, targets = batch\n",
        "            err = val_fn(inputs, targets)\n",
        "            val_err += err\n",
        "            val_batches += 1\n",
        "\n",
        "            # Then we print the results for this epoch:\n",
        "\n",
        "        \n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
        "        \n",
        "        t = train_err / train_batches\n",
        "        v = val_err / val_batches\n",
        "        print(\"  training loss:\\t\\t{:.10f}\".format(t))\n",
        "        print(\"  validation loss:\\t\\t{:.10f}\".format(v))\n",
        "        print(\"  gain of training loss:\\t\\t{:.10f}\".format(t0-t))\n",
        "        print(\"  gain validation loss:\\t\\t{:.10f}\".format(v0-v))\n",
        "\n",
        "        t0 = t\n",
        "        v0 = v\n",
        "        train_loss_curve.append(t)\n",
        "        val_loss_curve.append(v)\n",
        "\n",
        "\n",
        "\n",
        "        get_param_fn = theano.function([], params)        \n",
        "        suffix = '_ID'+identifier+'_date_'+str(num)\n",
        "        sio.savemat(output_folder+'loss'+suffix+'.mat',\n",
        "                    {'train_loss': np.asarray(train_loss_curve), 'val_loss': np.asarray(val_loss_curve)})\n",
        "                    \n",
        "        np.savez(output_folder+'model'+suffix+'.npz', *get_param_fn())\n",
        "        \n",
        " \n",
        "if __name__ == '__main__':\n",
        "    kwargs = {}\n",
        "    kwargs['data_folder'] = '/DATASET/'\n",
        "    kwargs['output_folder'] = '/MODEL/'\n",
        "    kwargs['identifier'] = 'OPTI'\n",
        "    kwargs['n_epochs'] = 10\n",
        "    main(kwargs['data_folder'], kwargs['output_folder'], kwargs['identifier'], kwargs['n_epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}